{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eGwNwDKEt8lG"
   },
   "source": [
    "#  06 - Supervised Learning Methods\n",
    "\n",
    "*HFT Stuttgart, 2024 Summer Term, Michael Mommert (michael.mommert@hft-stuttgart.de)*\n",
    "\n",
    "This Jupyter Notebook provides a simple introduction into Python programming and is based on Notebooks prepared by the amazing Dr. Marco Schreyer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nYpS4wEPt8lI"
   },
   "source": [
    "Today, we will use your Python skills implement our first machine learning models. For this purpose, we will utilize the [`scikit-learn`](https://scikit-learn.org/stable/index.html) package, which provides a huge amount of functionality for different machine learning tasks, as well as some datasets for learning how to use this functionality. \n",
    "\n",
    "In this example, we will utilize a *k*NN multi-class classfication model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D0Jnx-Ljt8lK"
   },
   "source": [
    "## Lab Objectives:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ybF-i5mQt8lL"
   },
   "source": [
    "The learning objectives for today are based on the supervised learning setup discussed in our lecture:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This Notebook follows this pipeline in its structure:\n",
    "\n",
    "> 0. Data loading and exploration\n",
    "> 1. Feature engineering\n",
    "> 2. Data scaling\n",
    "> 3. Data splitting\n",
    "> 4. Define hyperparameters\n",
    "> 5. Train model on fixed hyperparameters\n",
    "> 6. Evaluate model on val data set\n",
    "> 7. Maximize performance on validation data set by tuning hyperparameters\n",
    "> 8. Evaluate trained model on test data \n",
    "\n",
    "We will apply these steps to the $k$-NN classifier. Furthermore, we will look at the following things in this lab:\n",
    "> 9. Model evaluation with the confusion matrix\n",
    "> 10. Setting up a random forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CZaa0qAnt8lY"
   },
   "source": [
    "## -1. Setup of the Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2yTCqemyt8la"
   },
   "source": [
    "We install and import necessary packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "o3ShseCwt8lb",
    "outputId": "1254c7ff-5876-4508-8fde-5528e4d704f3"
   },
   "outputs": [],
   "source": [
    "# import the numpy, scipy and pandas data science library\n",
    "import numpy as np\n",
    "\n",
    "# import sklearn data and data pre-processing libraries\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# import matplotlib data visualization library\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_Z2tRqzFt8lu"
   },
   "source": [
    "Set a random seed for all our experiments - this ensures reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NzE1FzaSt8lu"
   },
   "outputs": [],
   "source": [
    "random_seed = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0gpZzk5t8l5"
   },
   "source": [
    "# 0. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5CtBrJGut8l9"
   },
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can extract the data, targets and target (class) names as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = iris.data\n",
    "y = iris.target\n",
    "y_classes = iris.target_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Explore the data yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 196
    },
    "colab_type": "code",
    "id": "kju1z4Cft8mk",
    "outputId": "cf9f8028-e60b-4acf-dfd1-c6b2aaed1ddd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lWofkTgQt8mw"
   },
   "source": [
    "Before we continue, we have a look at a method from Python's **Seaborn** library to create a pairwise plot of all features, referred to as a **Pairplot**. The Seaborn library is a powerful data visualization library based on the Matplotlib. It provides a great interface for drawing informative statstical graphics (https://seaborn.pydata.org). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 765
    },
    "colab_type": "code",
    "id": "JmfO2-yit8mx",
    "outputId": "6a2392f8-a12e-4360-a5a8-acdf6bc9970d"
   },
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(10, 10))\n",
    "\n",
    "# load the dataset also available in seaborn\n",
    "iris_plot = sns.load_dataset(\"iris\")\n",
    "\n",
    "# plot a pairplot of the distinct feature distributions\n",
    "sns.pairplot(iris_plot, diag_kind='hist', hue='species');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ugPoMiQt8m4"
   },
   "source": [
    "It can be observed from the created Pairplot, that most of the feature measurements that correspond to flower class \"setosa\" exhibit a nice **linear separability** from the feature measurements of the remaining flower classes. In addition, the flower classes \"versicolor\" and \"virginica\" exhibit a commingled and **non-linear separability** across all the measured feature distributions of the Iris Dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Feature Engineering\n",
    "\n",
    "Both the **input** data (`iris.data`) and the **output** data (`iris.target`) are already available in the form of quantitative data (continuous input data and discrete class labels), which we directly feed into our ML models. Therefore, no feature engineering is required for this specific data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data scaling\n",
    "\n",
    "We implement data scaling using the Standard scaler class, which forces upon the values in each feature a mean of unity and a spread that is based on the variance in the dataset. \n",
    "\n",
    "\n",
    "Let's use the standard scaler implemented in scikit-learn to scale our data. We have to import the correponding class and initialize it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To apply the scaler and retrieve a transformed dataset, use the `.fit_transform()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_scaled = scaler.fit_transform(iris.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('original data, mean =', np.mean(iris.data, axis=0))\n",
    "print('original data, std =', np.std(iris.data, axis=0))\n",
    "print('scaled data, mean =', np.mean(data_scaled, axis=0))\n",
    "print('scaled data, std =', np.std(data_scaled, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use the scaled data in our machine learning model. \n",
    "\n",
    "Hint: If you would like to undo the scaling tranformation, you can use the `.inverse_transform()` method of `scaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('original data:', iris.data[0,])\n",
    "print('scaled data:', data_scaled[0,])\n",
    "print('unscaled data:', scaler.inverse_transform(data_scaled[0,].reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: implement a different data scaler and compare the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oTBwny8Dt8m5"
   },
   "source": [
    "# 3. Data splitting\n",
    "\n",
    "To understand and evaluate the performance of any trained **supervised machine learning** model, it is good practice to divide the dataset into a **training dataset** (the fraction of data records solely used for training purposes), a **validation dataset** (data to evaluate the current settings of your hyperparameters) and a **test dataset** (the fraction of data records solely used for independent evaluation purposes). Please note that both the **validation dataset** and the **test dataset** will never be shown to the model as part of the training process. The **test dataset** is sometimes also referred to as **evaluation set**; both terms refer to the same concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YN25KKcvt8m6"
   },
   "source": [
    "We first split our scaled dataset into a training dataset and some other dataset (which we will refer to as *remainder* in the following) that will then be evenly split into a validation and test dataset. We set the fraction of training records to **60%** of the original dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kPFvlzS6t8m7"
   },
   "outputs": [],
   "source": [
    "train_fraction = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4FkQME8Ut8m9"
   },
   "source": [
    "Randomly split the scaled dataset into training set and evaluation set using sklearn's `train_test_split` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xF7m6KMSt8m9"
   },
   "outputs": [],
   "source": [
    "# 60% training and 40% remainder\n",
    "x_train, x_remainder, y_train, y_remainder = train_test_split(data_scaled, iris.target, test_size=1-train_fraction, \n",
    "                                                              random_state=random_seed, stratify=iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50% validation and 50% test\n",
    "x_val, x_test, y_val, y_test = train_test_split(x_remainder, y_remainder, test_size=0.5, \n",
    "                                                random_state=random_seed, stratify=y_remainder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T37IuZHIt8m_"
   },
   "source": [
    "Note the use of the `stratify` keyword argument here: a stratified split makes sure that approximately the same fraction of samples from each class is present in each dataset. Therefore, we have to provide the same list of class labels to this argument.\n",
    "\n",
    "Evaluate the different dataset dimensionalities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 33
    },
    "colab_type": "code",
    "id": "N9i0U2uzt8nA",
    "outputId": "65cba01c-5c0e-4e75-e66d-e92cbdff8e29"
   },
   "outputs": [],
   "source": [
    "print('original:', iris.data.shape, iris.target.shape)\n",
    "print('train:', x_train.shape, y_train.shape)\n",
    "print('val:', x_val.shape, y_val.shape)\n",
    "print('test:', x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Define Hyperparameters\n",
    "\n",
    "As we learned in the lecture, our *k*NN model has a single hyperparameter: the number of neighbors, *k*. We start by considering a simple *nearest neighbor* model, which, of course, implies that $k=1$.\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9HtRmw-t8nJ"
   },
   "source": [
    "# 5. Train model on fixed hyperparameters\n",
    "\n",
    "We start by creating a model instance, which requires passing the chosen hyperparameters to the model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have to train the model on our training dataset. Each model implemented in `scikit-learn` has a `.fit()` method for this purpose. \"Fitting\" refers here to the same idea that we typically refer to as \"training\", so don't get confused.\n",
    "\n",
    "The training of the data requires two `arrays`: the training input features ($\\mathbf{X}$) and the training target vector ($\\mathbf{y}$), such that for a given classifier $f$ the following holds: \n",
    "$$f(\\mathbf{X}) = \\mathbf{y}$$\n",
    "\n",
    "The way we split our dataset and into `x_train` and `y_train` already follows this naming convention. We can use those `arrays` readily in the training. Just for reference: `x_train` has to be of shape `(n_samples, n_features)` and `y_train` has to be of shape `(n_samples,)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`model` is now trained and can be used to make predictions. Let's take one datapoint from our training dataset and see whether it makes a correct prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([x_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, it classifies this single datapoint correctly. However, this is not a good way to test or evaluate the performance of your model. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluate model on val data set\n",
    "\n",
    "Of course, we should use our previously split test sample for evaluating our model performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_val)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A quick by-eye check seems to look pretty promising, but of course we need a more quantitative metric for the performance of our model.\n",
    "\n",
    "In the case of classification, we can use the accuracy metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y_pred, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is it. After evaluation on our independent eval dataset - which the model has not seen during training - we find that our model makes an accurate prediction in 96.7% of cases.\n",
    "\n",
    "This could be it, but there is a good chance that by tuning our sole hyperparameter, $k$, we can achieve a better result. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Maximize performance on validation data set by tuning hyperparameters\n",
    "\n",
    "Let's compile all the relevant code in one cell and try a different value for $k$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KNeighborsClassifier(n_neighbors=k)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_val)\n",
    "accuracy_score(y_pred, y_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use a loop over different choices for $k$ and evaluate the model for these parameters to find the best-performing one. This process is called **hyperparameter tuning**.\n",
    "\n",
    "However, there is one more technical detail. Currently, we evaluate the performance on our **test dataset**. If we select $k$ based on these evaluations and therefore the **test dataset**, we have a *data leakage*. To resolve that issue, we can evaluate our model on the **validation dataset** for different $k$s and then, after picking the best-performing $k$, we can evaluate that model on the **test dataset**, providing an independent measure of performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in [1, 3, 5, 7, 10, 15, 20]:\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(x_train, y_train)\n",
    "    y_pred = model.predict(x_val)\n",
    "    print('k={:d}, val accuracy={:.2f}%'.format(k, accuracy_score(y_pred, y_val)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that the model performs equally well for $k\\sim3$ and $k\\sim10$. Based on experience, I would pick $k=10$. Why? For small values of $k$, you are more likely to **overfit** the training data, so choosing a larger value of $k$ increases the chances that the model generalizes well to data it has never seen before.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 8. Evaluate trained model on test data set\n",
    "\n",
    "Let's retrain the model with $k=10$, make predictions on the test data set and we're done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "model = KNeighborsClassifier(n_neighbors=k)\n",
    "model.fit(x_train, y_train)\n",
    "y_pred = model.predict(x_test)\n",
    "print('final test accuracy={:.2f}%'.format(accuracy_score(y_pred, y_test)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, evaluating the model on the test dataset provides the same accuracy as for the validation dataset - but this is not always the case, since both datsets are different from one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Model evaluation with the confusion matrix\n",
    "\n",
    "So far, we have only considered the accuracy metric to evaluate our predictions. It would be useful to know whether one class of iris is more likely to be mistaken than another. For that purpose, confusion matrizes are used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "mat = confusion_matrix(y_test, y_pred)\n",
    "mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the y-axis you have your predicted classes, on the x-axis you have the actual classes. Those entries on the diagonal have been accurately predicted. Entries off the diagonal indicate how many flowers have incorrect class predictions.\n",
    "\n",
    "The ``seaborn`` library provides a method to generate nicely formatted confusion matrizes. Let's give it a try:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init the plot\n",
    "plt.figure(figsize=(7, 7))\n",
    "\n",
    "# plot confusion matrix heatmap\n",
    "sns.heatmap(mat, square=True, annot=True, fmt='d', cbar=False, cmap='Reds', \n",
    "            xticklabels=iris.target_names, yticklabels=iris.target_names)\n",
    "\n",
    "# add plot axis labels\n",
    "plt.xlabel('Ground Truth')\n",
    "plt.ylabel('Prediction')\n",
    "\n",
    "# add plot title\n",
    "plt.title('Confusion Matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Setting up a random forest classifier\n",
    "\n",
    "With `scikit-learn`, it is easy to simpoly try out a different model, since all algorithms (ML methods, scalers, etc.) are implemented as classes and thus provide `.fit()` and `.transform()` methods.\n",
    "\n",
    "**Exercise**: Implement a random forest classifier! Here, we use two different hyperparameters: the number of trees in the ensemble and the maximum depth of the individual trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "eGwNwDKEt8lG",
    "D0Jnx-Ljt8lK",
    "CZaa0qAnt8lY",
    "mMSfpCPvt8l4",
    "n94u0rxat8su"
   ],
   "name": "aiml_NB.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
